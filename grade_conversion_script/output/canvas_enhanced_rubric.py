import enum
from itertools import chain
import pandas as pd
from pathlib import Path

from typing import * # pyright: ignore[reportWildcardImportFromLibrary]
import numbers as num
import pandera.pandas as pa
from numpy.ma.extras import row_stack
from pandera.typing import DataFrame

from .base import OutputFormat

from grade_conversion_script.util import AliasRecord
from grade_conversion_script.util.types import SisId, StudentPtsById
from grade_conversion_script.util.funcs import associate_unrecognized_entities, best_effort_is_name, contains_row_for, iter_by_element, is_pd_value_present, reindex_to
from grade_conversion_script.util.tui import interactive_rubric_criteria_match, interactive_alias_match

class CanvasEnhancedRubricOutputFormat(OutputFormat):
    '''
    Populate an Enhanced Rubric file generated by Canvas.

    Does not consider Rating label when determining
    whether an action would overwrite an existing grade.

    >>> name_sis_id = NameSisIdConverter()
    >>> name_sis_id.add(sis_id='name1', name='Name One')
    >>> rubric_csv = pd.DataFrame({
    ...                         'Student Name':           ["Name One",],
    ...                         'Criterion 1 - Rating':   ["Any Text",],
    ...                         'Criterion 1 - Points':   ["",        ],
    ...                         'Criterion 1 - Comments': ["",        ],
    ...                         'Criterion 2 - Rating':   ["",        ],
    ...                         'Criterion 2 - Points':   [5,         ],
    ...                         'Criterion 2 - Comments': ["excused", ],
    ...                     })
    >>> cerubric_output = CanvasEnhancedRubricOutputFormat(
    ...                         rubric_csv,
    ...                         name_sis_id,
    ...                         name_sis_id_match=lambda out, names_to_match, sis_ids_to_match: Exception(
    ...                                                 f"{out};{names_to_match};{sis_ids_to_match}"
    ...                                             ),
    ...                         rubric_criteria_match=( lambda given_labels, dest_labels: dict(zip(given_labels, dest_labels)) ),
    ...                         replace_existing=False,
    ...                         warn_existing=True
    ...                     )
    >>> grades = pd.DataFrame({
    ...                         'sis_id':      ['name1',],
    ...                         'Criterion 1': [3,      ],
    ...                         'Criterion 2': [4,      ]
    ...                     }).set_index('sis_id', drop=True)
    >>> new_csv = cerubric_output.format(grades)
    Keeping old grade of 5 at name1, Criterion 2 (Rating "" and Comment "excused")
    >>> new_csv[['Student Name', 'Criterion 1 - Points', 'Criterion 2 - Points']]
          Student Name Criterion 1 - Points  Criterion 2 - Points
    name1     Name One                    3                     5
    '''

    class CriterionField(enum.Enum):
        PTS_LABEL = " - Rating"
        PTS = " - Points"
        COMMENTS = " - Comments"

        def col_name_for(self, criterion: str) -> str:
            return criterion + self.value

        @classmethod
        def remove_field_suffix(cls, col_name: str) -> str:
            for field in cls:
                if col_name.endswith(field.value):
                    return col_name.removesuffix(field.value)
            raise ValueError(f"{col_name} does not end in known field suffix")

    def __init__(
            self,
            rubric_csv: pd.DataFrame,
            student_aliases: AliasRecord,
            unrecognized_name_match=interactive_alias_match,
            rubric_criteria_match=interactive_rubric_criteria_match,
            *,
            replace_existing: bool,
            warn_existing: bool
        ):
        super().__init__(student_aliases)

        self.rubric_template = rubric_csv
        
        self.replace_existing = replace_existing
        self.warn_existing = warn_existing

        self.unrecognized_name_match = unrecognized_name_match
        self.rubric_criteria_match = rubric_criteria_match

    def merge_conflict_values(self, existing: pd.DataFrame, incoming: pd.DataFrame, index_to_alias_id: pd.Series, full_existing: pd.DataFrame) -> tuple[pd.DataFrame, str | None]:
        '''
        Columns of `existing` and `incoming` have ' - Points' suffix'.
        '''
        # constants
        subline_start = "\n    "
        tab = "\t"
        def conflicts_detail():
            for row, col, new_val in iter_by_element(incoming):
                if pd.isna(new_val):
                    continue
                assert isinstance(row, (num.Real, str))
                assert isinstance(col, str)

                existing_val = existing.loc[row, col]
                alias_id: int = index_to_alias_id[row]
                student_name = self.student_aliases.best_effort_alias(best_effort_is_name, id=alias_id)
                criterion = self.CriterionField.remove_field_suffix(col)

                rating= full_existing.loc[row, criterion + self.CriterionField.PTS_LABEL.value]
                comment = full_existing.loc[row, criterion + self.CriterionField.COMMENTS.value]

                yield (existing_val, new_val, student_name, criterion, rating, comment)

        assert existing.index.equals(incoming.index)
        assert len(existing.columns) == len(incoming.columns)
        assert all(x.endswith(" - Points") for x in chain(existing.columns, incoming.columns))

        if existing.empty:
            return (existing, None)

        if self.replace_existing:
            values = incoming
            message = subline_start.join((
                f"Replacing existing grade values (keeping values for Rating and Comment):",
                *(
                    tab.join((
                        f"{existing_val}",
                        f"(new: {new_val},",
                        f"student: {student_name},",
                        f"criterion: {criterion})",
                        f"(Rating: {rating},",
                        f"Comment: {comment})",
                    ))
                    for existing_val, new_val, student_name, criterion, rating, comment
                    in conflicts_detail()
                )
            ))
        else:
            values = existing
            message = subline_start.join((
                f"Keeping existing grade values:",
                *(
                    tab.join((
                        f"{existing_val}",
                        f"(student: {student_name},",
                        f"criterion: {criterion})",
                        f"(Comment: {comment})",
                    ))
                    for existing_val, _, student_name, criterion, _, comment
                    in conflicts_detail()
                )
            ))

        return (values, message if self.warn_existing else None)

    @override
    @pa.check_types
    def format(self, grades: DataFrame[StudentPtsById]) -> pd.DataFrame:
        '''
        Args:
            grades:
                One column per rubric criteria.
                One row per student.
        Returns:
            A dataframe which can be saved to file with self.write_file().
        '''

        new_rubric = self.rubric_template.copy()

        # Perform name matching
        associate_unrecognized_entities(
            self.student_aliases,
            self.unrecognized_name_match,
            input_ids=grades.index,
            dest_alias_lists=new_rubric['Student Name']
        )
        # matching is applied inside of self.student_aliases

        # Perform matching: input column label -> rubric criterion
        user_criteria = grades.columns.to_list()
        canvas_criteria = (
            self.CriterionField.remove_field_suffix(col_name)
            for col_name in new_rubric.columns
            if col_name.endswith(self.CriterionField.PTS.value) # pull one column label for each criterion
        )
        given_to_dest_criteria = self.rubric_criteria_match(
            given_labels=user_criteria,
            dest_labels=canvas_criteria
        )
        grades.columns = grades.columns.map(given_to_dest_criteria)

        # Reindex grades to align with output
        id_by_rubric_row = self.student_aliases.id_of_df(
            new_rubric,
            'Student Name',
            expect_new_entities=True,
            collect_new_aliases=True
        )
        incoming_grades_aligned_rows = reindex_to(
            to_realign=grades,
            target_ids=id_by_rubric_row
        )

        # Resolve + Set values (handle replace_existing, warn_existing)

        # determine which elements are conflicting
        determines_existing = (self.CriterionField.PTS, self.CriterionField.COMMENTS)
        # `has_existing` and `has_incoming` have
        # index like `new_rubric` but
        # columns like `incoming_grades_aligned_rows`.
        has_existing = (
            new_rubric.groupby( # pyright: ignore[reportUnknownMemberType, reportCallIssue]
                lambda col: self.CriterionField.remove_field_suffix(col),
                as_index=True,
                sort=False,
                axis='columns'
            )
            .apply(
                lambda group: group.notna().all(axis="columns"),
            )
        )
        has_incoming: pd.DataFrame = (
            incoming_grades_aligned_rows
            .notna()
            .reindex(
                fill_value=bool(False),
                index=new_rubric.index
            )
        )

        # rename incoming grades to end with ' - Points' for convenience
        incoming_grades_aligned_all = incoming_grades_aligned_rows.reindex(
            columns=incoming_grades_aligned_rows.columns.map(
                lambda col_name: self.CriterionField.PTS.col_name_for(col_name)
            )
        )

        # set conflicting
        conflicting = has_existing & has_incoming
        conflict_vals, warning_msg = self.merge_conflict_values(
            existing = new_rubric.where(conflicting),
            incoming = incoming_grades_aligned_all.where(conflicting),
            index_to_alias_id = id_by_rubric_row,
            full_existing = new_rubric
        )
        new_rubric.mask(conflicting, other=conflict_vals, inplace=True)
        if warning_msg:
            print(warning_msg)

        # set non-conflicting
        non_conflicting = (~has_existing) & has_incoming
        non_conflict_vals = incoming_grades_aligned_all[non_conflicting]
        new_rubric.mask(non_conflicting, other=non_conflict_vals, inplace=True)

        # Return, asserts

        # We only touched "Points" columns that were specified in the input
        modified_columns: list[str] = [f"{criterion} - Points"
                                       for criterion in grades.columns]
        assert (
            self.rubric_template[[*new_rubric.columns]]
                .drop(modified_columns, axis='columns', inplace=False)
        ).equals(
            new_rubric
                .drop(modified_columns, axis='columns', inplace=False)
        )

        return new_rubric
    
    @override
    @classmethod
    def write_file(cls, self_output: pd.DataFrame, filepath: Path) -> None:
        self_output.to_csv(filepath, index=False, header=True)